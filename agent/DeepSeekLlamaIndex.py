from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
from llama_index.llms.deepseek import DeepSeek
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import os

# è®¾ç½® DeepSeek API
from dotenv import load_dotenv
load_dotenv()

# é…ç½® LLM å’Œ Embedding
Settings.llm = DeepSeek(model="deepseek-chat", api_key=os.environ["DEEPSEEK_API_KEY"])
Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-small-en-v1.5")


def basic_qa_demo():
    """åŸºç¡€é—®ç­”demo"""
    print("=== DeepSeek + LlamaIndex åŸºç¡€é—®ç­” Demo ===")

    # åˆ›å»ºä¸€äº›ç¤ºä¾‹æ–‡æ¡£å†…å®¹
    sample_docs = """
    DeepSeek æ˜¯ä¸€å®¶é¢†å…ˆçš„äººå·¥æ™ºèƒ½å…¬å¸ï¼Œä¸“æ³¨äºå¤§è¯­è¨€æ¨¡å‹ç ”å‘ã€‚
    DeepSeek-V3 æ˜¯å…¬å¸æœ€æ–°å‘å¸ƒçš„å¤§è¯­è¨€æ¨¡å‹ï¼Œæ”¯æŒ128Kä¸Šä¸‹æ–‡é•¿åº¦ã€‚
    å…¬å¸æˆç«‹äº2023å¹´ï¼Œæ€»éƒ¨ä½äºä¸­å›½ã€‚
    DeepSeek æ¨¡å‹åœ¨ä»£ç ç”Ÿæˆã€è‡ªç„¶è¯­è¨€ç†è§£æ–¹é¢è¡¨ç°ä¼˜å¼‚ã€‚
    """

    # å°†æ–‡æœ¬å†…å®¹å†™å…¥ä¸´æ—¶æ–‡ä»¶
    with open("sample_doc.txt", "w", encoding="utf-8") as f:
        f.write(sample_docs)

    # åŠ è½½æ–‡æ¡£
    documents = SimpleDirectoryReader(input_files=["sample_doc.txt"]).load_data()

    # åˆ›å»ºå‘é‡ç´¢å¼•
    index = VectorStoreIndex.from_documents(documents)

    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    query_engine = index.as_query_engine()

    # è¿›è¡ŒæŸ¥è¯¢
    questions = [
        "DeepSeek æ˜¯ä»€ä¹ˆå…¬å¸ï¼Ÿ",
        "DeepSeek-V3 æ”¯æŒå¤šé•¿çš„ä¸Šä¸‹æ–‡ï¼Ÿ",
        "DeepSeek åœ¨å“ªäº›æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Ÿ"
    ]

    for question in questions:
        print(f"\nğŸ¤” é—®é¢˜: {question}")
        response = query_engine.query(question)
        print(f"ğŸ¤– å›ç­”: {response}")
        print("-" * 50)


if __name__ == "__main__":
    basic_qa_demo()